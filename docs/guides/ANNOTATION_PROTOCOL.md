# Ground Truth Annotation Protocol for RegClassifier

**Version:** 1.0
**Date:** 2026-02-03
**Purpose:** Guide for creating high-quality human-labeled evaluation sets

---

## Table of Contents

1. [Overview](#overview)
2. [Why Ground Truth Annotation is Critical](#why-critical)
3. [Annotation Scope and Goals](#scope)
4. [Label Definitions](#labels)
5. [Annotation Process](#process)
6. [Quality Control](#quality)
7. [Tools and Resources](#tools)
8. [Timeline and Budget](#timeline)

---

## 1. Overview <a name="overview"></a>

This protocol defines the process for creating ground-truth labeled datasets for evaluating the RegClassifier system. Ground-truth labels are **human-annotated** labels that serve as the gold standard for measuring model performance.

**Key Principle:** Weak labels (from keyword matching) are used ONLY for training. Evaluation MUST use human-annotated ground truth to avoid data leakage.

---

## 2. Why Ground Truth Annotation is Critical <a name="why-critical"></a>

### Current Problem: Data Leakage

The system currently suffers from **circular validation**:
1. Weak labels generated by keyword rules (e.g., "IRB" → IRB label)
2. Model trained on weak labels
3. **Model evaluated using the SAME weak labels** ❌

**Result:** Metrics (Recall@10, mAP, nDCG@10) measure agreement with noisy training labels, NOT true performance.

### Solution: Ground Truth Labels

1. Human experts manually label 1000-2000 chunks
2. Use weak labels ONLY for training
3. Evaluate ONLY on human-annotated test set ✓

**Impact:** Valid, publishable performance metrics

---

## 3. Annotation Scope and Goals <a name="scope"></a>

### Target Dataset Size

**Minimum (Research):**
- **Development Set:** 500 labeled chunks (for threshold tuning, hyperparameter search)
- **Test Set:** 500 labeled chunks (held-out for final evaluation)
- **Total:** 1000 chunks

**Recommended (Production):**
- **Development Set:** 750 chunks
- **Test Set:** 1250 chunks
- **Total:** 2000 chunks

### Label Coverage

**All 14 regulatory topic labels:**
1. IRB (Internal Ratings Based)
2. CreditRisk
3. Securitisation
4. SRT (Significant Risk Transfer)
5. MarketRisk_FRTB
6. Liquidity_LCR
7. Liquidity_NSFR
8. LeverageRatio
9. OperationalRisk
10. AML_KYC (Anti-Money Laundering / Know Your Customer)
11. Governance
12. Reporting_COREP_FINREP
13. StressTesting
14. Outsourcing_ICT_DORA

**Target:** Minimum 50 positive examples per label (balanced representation)

### Graded Relevance (Optional but Recommended)

Instead of binary labels (relevant / not relevant), use **3-level graded relevance**:

- **2 (Highly Relevant):** Chunk directly addresses the topic, core content
- **1 (Somewhat Relevant):** Chunk mentions the topic but not central focus
- **0 (Not Relevant):** Chunk is about a different topic

**Benefit:** Enables proper nDCG evaluation with graded judgments

---

## 4. Label Definitions <a name="labels"></a>

### Annotation Guidelines

For each label, annotators must understand:
- **Definition:** What the regulatory topic covers
- **Inclusion Criteria:** When to apply the label
- **Exclusion Criteria:** When NOT to apply the label
- **Examples:** Positive and negative examples

### Example: IRB (Internal Ratings Based)

**Definition:**
Internal Ratings Based approach for measuring credit risk, where banks use internal models to estimate PD (Probability of Default), LGD (Loss Given Default), and EAD (Exposure at Default).

**Inclusion Criteria:**
- Chunk discusses IRB model requirements, calibration, validation
- Mentions PD, LGD, EAD calculations for IRB
- References specific IRB articles (e.g., Article 143-191 of CRR)
- Discusses IRB vs. Standardised Approach comparisons
- Covers IRB permission/qualification requirements

**Exclusion Criteria:**
- Generic credit risk (use CreditRisk label instead)
- Standardised Approach only (not IRB-specific)
- Market risk, operational risk (different risk types)
- General risk management without IRB specifics

**Positive Examples:**
✓ "Institutions using the IRB approach shall estimate PD for each rating grade..."
✓ "The calculation of risk-weighted exposure amounts under the IRB approach..."
✓ "IRB permission requirements include robust internal governance..."

**Negative Examples:**
✗ "Credit institutions shall calculate credit risk capital requirements..." (too generic → CreditRisk)
✗ "Market risk models must be validated annually..." (different topic → MarketRisk_FRTB)
✗ "The leverage ratio is calculated as..." (different topic → LeverageRatio)

### Example: Liquidity_LCR

**Definition:**
Liquidity Coverage Ratio (LCR) requires banks to hold sufficient high-quality liquid assets (HQLA) to survive a 30-day stress scenario.

**Inclusion Criteria:**
- LCR calculation methodology
- HQLA definitions and classifications
- LCR reporting and monitoring
- Net cash outflows under stress
- LCR thresholds and requirements

**Exclusion Criteria:**
- NSFR (use Liquidity_NSFR label)
- General liquidity without LCR specifics
- Funding or capital adequacy (different topics)

**Positive Examples:**
✓ "The LCR shall be calculated as the stock of HQLA divided by total net cash outflows..."
✓ "Level 1 assets include cash and central bank reserves..."
✓ "LCR reporting must be submitted monthly to the competent authority..."

**Negative Examples:**
✗ "The NSFR is calculated as available stable funding / required stable funding..." (different → Liquidity_NSFR)
✗ "Banks must maintain adequate liquidity buffers..." (too generic)

*[Repeat similar guidelines for all 14 labels]*

---

## 5. Annotation Process <a name="process"></a>

### Phase 1: Annotator Training (1 week)

**Goals:**
- Annotators understand all 14 label definitions
- Practice on 50 sample chunks with known labels
- Achieve inter-annotator agreement (IAA) ≥ 0.7 (Fleiss' kappa) on training set

**Activities:**
1. Review annotation guidelines (this document)
2. Study regulatory documents (CRR, Basel III, EBA guidelines)
3. Practice annotation on 50 pre-labeled chunks
4. Discuss disagreements and clarify edge cases
5. Refine guidelines based on feedback

### Phase 2: Pilot Annotation (1 week)

**Goals:**
- Validate annotation process on small sample
- Measure IAA on real data
- Identify problematic labels or chunks

**Activities:**
1. 3 annotators independently label 100 chunks
2. Compute Fleiss' kappa per label
3. Identify low-agreement labels (kappa < 0.6)
4. Hold adjudication meeting to resolve disagreements
5. Update guidelines if needed

**Quality Threshold:** Fleiss' kappa ≥ 0.7 (substantial agreement)

### Phase 3: Full Annotation (4-6 weeks)

**Workflow:**

**Step 1: Chunk Sampling**
- Sample 2000 chunks from full corpus
- Use stratified sampling to ensure label diversity
- Include both easy and hard examples

**Step 2: Annotation Assignment**
- Each chunk annotated by 2-3 independent annotators
- Use annotation tool (Prodigy, Label Studio, or custom)
- Randomize chunk order to avoid bias

**Step 3: Annotation Interface**

For each chunk, annotator sees:
- **Context:** Chunk text + surrounding chunks (optional)
- **Source:** Document ID and location
- **Labels:** Multi-select checkboxes for 14 labels
- **Relevance:** For each selected label, grade as 0/1/2 (optional)
- **Confidence:** Annotator confidence (low/medium/high)
- **Notes:** Free text for edge cases or questions

**Step 4: Quality Monitoring**
- Track annotation speed (chunks per hour)
- Monitor IAA on overlapping chunks (updated weekly)
- Flag chunks with high disagreement for review
- Hold weekly meetings to discuss difficult cases

**Step 5: Adjudication**
- For chunks with annotator disagreement:
  - 2 annotators agree → use majority vote
  - No majority → senior annotator adjudicates
  - Very difficult → exclude from dataset (document reason)

### Phase 4: Dataset Finalization (1 week)

**Activities:**
1. Compile all annotations
2. Compute final IAA statistics (Fleiss' kappa, Cohen's kappa)
3. Split into dev (40%) and test (60%) sets
4. Use stratified split to preserve label distribution
5. Document dataset characteristics (label frequencies, chunk lengths, etc.)
6. Archive annotations with version control (v1.0)

**Final Deliverables:**
- `dev_set_v1.0.csv` (500-750 chunks)
- `test_set_v1.0.csv` (500-1250 chunks)
- `annotation_report_v1.0.pdf` (IAA statistics, dataset characteristics)
- `annotation_guidelines_v1.0.md` (finalized guidelines)

---

## 6. Quality Control <a name="quality"></a>

### Inter-Annotator Agreement (IAA)

**Primary Metric:** Fleiss' kappa (for ≥3 annotators) or Cohen's kappa (for 2 annotators)

**Interpretation:**
- **κ < 0.20:** Slight agreement (unacceptable)
- **κ 0.21-0.40:** Fair agreement (needs improvement)
- **κ 0.41-0.60:** Moderate agreement (acceptable for difficult tasks)
- **κ 0.61-0.80:** Substantial agreement (good)
- **κ 0.81-1.00:** Almost perfect agreement (excellent)

**Target:** κ ≥ 0.70 overall, κ ≥ 0.60 per label

### Quality Checks

**1. Annotation Speed:**
- Expected: 5-10 chunks per hour (with graded relevance)
- Too fast (>15/hour) → possible lack of care
- Too slow (<3/hour) → provide additional training

**2. Label Distribution:**
- Ensure no label has <50 positive examples
- Flag labels with very high (>80%) or very low (<5%) prevalence
- Compare to weak label distribution (should differ)

**3. Annotator Consistency:**
- Compute per-annotator agreement with majority vote
- Identify outlier annotators (low agreement)
- Provide feedback or additional training

**4. Edge Case Documentation:**
- Maintain list of difficult chunks with rationale
- Update guidelines to address common edge cases
- Create FAQ for annotators

---

## 7. Tools and Resources <a name="tools"></a>

### Annotation Tools

**Option 1: Prodigy (Recommended)**
- Pros: Designed for NLP, active learning, efficient UI
- Cons: Commercial license required (~$390/user)
- Setup: https://prodi.gy/docs/install

**Option 2: Label Studio (Open Source)**
- Pros: Free, open-source, multi-modal, Docker deployment
- Cons: Requires setup, less NLP-specific
- Setup: https://labelstud.io/guide/install.html

**Option 3: Custom MATLAB GUI**
- Pros: Integrated with existing codebase
- Cons: Requires development effort
- Effort: ~2-3 days to build basic interface

### Annotator Resources

**Regulatory Documents:**
- Capital Requirements Regulation (CRR) - EUR-Lex
- Basel III framework - BIS website
- EBA Technical Standards - EBA website
- Glossary of regulatory terms

**Training Materials:**
- Annotation guidelines (this document)
- Label definition cards (1-page summaries per label)
- Example annotations with explanations
- FAQ for common edge cases

---

## 8. Timeline and Budget <a name="timeline"></a>

### Timeline Estimate

| Phase | Duration | Activities |
|-------|----------|------------|
| Annotator Training | 1 week | Guidelines review, practice annotation |
| Pilot Annotation | 1 week | 100 chunks, IAA measurement, guideline refinement |
| Full Annotation | 4-6 weeks | 2000 chunks, 2-3 annotators per chunk |
| Dataset Finalization | 1 week | Adjudication, splitting, documentation |
| **Total** | **7-9 weeks** | |

### Effort Estimate

**Per Chunk:**
- Annotation time: 6-12 minutes per annotator (avg 8 min)
- 3 annotators per chunk → 24 min total
- Adjudication: 5 min for 30% of chunks

**Total Effort:**
- 2000 chunks × 24 min = 800 hours (annotation)
- 600 chunks × 5 min = 50 hours (adjudication)
- Training & meetings: 40 hours
- **Total: ~890 hours**

**FTE Calculation:**
- 3 annotators × 20 hours/week = 60 hours/week
- 890 hours / 60 hours/week = ~15 weeks (includes overhead)
- **Matches 7-9 week timeline with 3 annotators**

### Budget Estimate

**Option A: Internal Annotators (Banking Regulatory Experts)**
- 3 experts × $100/hour × 300 hours each = $90,000
- Annotation tool: $1,200 (Prodigy licenses)
- **Total: ~$91,000**

**Option B: External Annotation Service**
- Specialized legal/compliance annotators: $50/hour
- 900 hours × $50/hour = $45,000
- Project management: $5,000
- Annotation tool: $1,200
- **Total: ~$51,000**

**Option C: Hybrid (Pilot Internal, Full External)**
- Internal experts for training & pilot: 100 hours × $100/hour = $10,000
- External service for full annotation: 800 hours × $40/hour = $32,000
- **Total: ~$42,000**

**Recommended:** Option C (Hybrid) for balance of quality and cost

---

## Appendix A: Annotation Interface Mockup

```
=== Chunk Annotation Interface ===

Document: CRR_Article_143.pdf
Chunk ID: CH_001_0

Context (previous):
"...capital requirements for credit risk..."

**[CURRENT CHUNK]**
"Institutions using the IRB approach shall estimate the probability of default (PD)
for each obligor grade, the loss given default (LGD), and exposure at default (EAD)
using their internal models, subject to supervisory approval."

Context (next):
"...The PD shall be calculated as a 12-month default probability..."

---

Select applicable labels (multi-select):

☐ IRB (Internal Ratings Based)
☐ CreditRisk
☐ Securitisation
☐ SRT (Significant Risk Transfer)
☐ MarketRisk_FRTB
☐ Liquidity_LCR
☐ Liquidity_NSFR
☐ LeverageRatio
☐ OperationalRisk
☐ AML_KYC
☐ Governance
☐ Reporting_COREP_FINREP
☐ StressTesting
☐ Outsourcing_ICT_DORA

For each selected label, grade relevance:
IRB: ● Highly Relevant  ○ Somewhat Relevant  ○ Not Relevant

Confidence: ○ Low  ● Medium  ○ High

Notes (optional):
__________________________________________________________

[Previous] [Save & Next] [Flag for Review]
```

---

## Appendix B: Quality Control Checklist

**Before Annotation:**
- [ ] All annotators completed training
- [ ] Pilot IAA ≥ 0.7 achieved
- [ ] Annotation tool configured and tested
- [ ] Sampling strategy approved

**During Annotation (Weekly):**
- [ ] IAA computed on overlapping chunks
- [ ] Annotation speed monitored
- [ ] Edge cases documented
- [ ] Annotator questions addressed

**After Annotation:**
- [ ] Final IAA ≥ 0.7 achieved
- [ ] All chunks adjudicated
- [ ] Dev/test split completed (stratified)
- [ ] Dataset documentation finalized
- [ ] Annotations archived with version control

---

## References

- Artstein & Poesio 2008 - "Inter-Coder Agreement for Computational Linguistics"
- Hripcsak & Rothschild 2005 - "Agreement, the F-Measure, and Reliability in Information Retrieval"
- Voorhees & Harman 2005 - "TREC: Experiment and Evaluation in Information Retrieval"

---

**Document Prepared By:** Claude Code (AI Assistant)
**Session:** https://claude.ai/code/session_01J7ysVTBVQFvZzSiELoBvki
**Branch:** claude/methodological-review-5kflq
